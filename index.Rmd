---
title: "Modelos de regresión"
author:
- Ana Xiangning Pereira Ezquerro
- Pedro Redondo Loureiro
- João Víctor Teodoro Rodrigues
- Diego Valcarce Ríos
output: 
  prettydoc::html_pretty:
    theme: leonids
    fig_caption: yes
    highlight: tango
    toc: yes
    fig_width: 10 
    fig_height: 5
---

```{r configuraciones, include=F}
knitr::opts_chunk$set(
  eval=F
)
```

Renuncia de responsibilidad.

# Regresión lineal simple
Objetivo: construir un modelo tal que $Y_i = \beta_0 + \beta_1\cdot X_i + \epsilon_i$

```{r include = FALSE}
data("iris")
var.endogena <- iris[,1]
var.explicativa <- iris[,2]
datos <- data.frame(cbind(var.endogena, var.explicativa))
library(DAAG)
library(stats)
library(MASS)
library(car)
library(lmtest)
library(tseries)
library(caret)
valores.predecir <- c(1, 2, 3)
```


## Construcción del modelo
```{r}
ajuste <- lm(var.endogena~var.explicativa, data = datos)

coeficientes <- ajuste$coefficients
valores.ajustados <- ajuste$fitted.values
residuos <- ajuste$residuals
gl.residuos <- ajuste$df.residual # grados de libertad de los residuos
gl.modelo <- ajuste$rank          # grados de libertad del modelo
SSR <- sum(residuos^2)            # suma de residuos al cuadrado
MSSR <- SSR/gl.residuos           # media de SSR
leverage <- hatvalues(ajuste)
n.equivalente.datos <- 1/leverage
```

## Inferencia
### Intervalos de confianza
```{r}
intervalo.parametros <- confint(ajuste, level = 0.95)
intervalo.varianza <- gl.residuos * MSSR / qchisq(p=0.10,df=gl.residuos)
```
### Contrastes de hipótesis
```{r}
# Contraste de regresión (H_0: beta_1 = 0):
resumen <- summary(ajuste)
contraste <- resumen$coefficients
# en la última columna tenemos el contraste de regrsión y el contraste de
# beta_0 = 0

# Podemos acceder al resultado del mismo contraste mediante anova:
contraste.regresion <- anova(ajuste)
```

## Bondad del ajuste
```{r results = 'hide', fig.show='hide'}
# Coeficiente de correlación lineal de Pearson estimado:
r <- cor(var.endogena, var.explicativa)
# ¿Es significativamente diferente de cero? H0:rho=0 vs H1:rho!=0
cor.test(var.endogena, var.explicativa, method = "pearson")

# Coeficiente de determinación
R2 <- resumen$r.squared
R2.ajustado <- resumen$adj.r.squared

# AIC, Criterio de Información de Akaike
AIC(ajuste)    # cuanto más pequeño, mejor (más ajustado)
# BIC, Criterio de Información Bayesiano (lo mismo, pero teniendo en cuenta
# el número de datos que tenemos)
BIC(ajuste)    # cuanto más pequeño, mejor (más ajustado)

# Validación cruzada
# hace un plot además con los 'distintos' modelos creados
cvRes <- DAAG::CVlm(data=datos, form.lm=var.endogena~var.explicativa,
                    m=5, dots=FALSE, seed=29, plotit="Observed", 
                    legend.pos="topleft", 
                    printit = FALSE, main="")

```

## Diagnosis del modelo
```{r}
# Residuos del ajuste
res.est  <- MASS::stdres(ajuste)    # estandarizados
res.stud <- MASS::studres(ajuste)   # estudentizados 
```

Vamos a corroborar las hipótesis estructurales del modelo: linealidad, homoscedasticidad
normalidad y aleatoriedad. Con `plot(ajuste)` tenemos acceso a diversas gráficas
que pueden ser de ayuda:

||¿Qué representa?|¿Qué comprobamos?|
|-------|:-------:|:-----:|
|`plot(ajuste, which = 1)`|Valores ajustados ~ Residuos|Linealidad|
|`plot(ajuste, which = 2)`|QQ Plot|Normalidad|
|`plot(ajuste, which = 3)`|Valores ajustados ~ Residuos estandarizados|Homoscedasticidad|
|`plot(ajuste, which = 4)`|Distancias de Cook|[Datos influyentes](#influence)|
|`plot(ajuste, which = 5)`|Leverage ~ Residuos estandarizados|[Datos influyentes](#influence)|

Pruebas analíticas:

### Linealidad
Contraste de linealidad. 
$$
\left\lbrace
\begin{matrix}
H_0&:& \mathbb{E}(\text{var.endogena}_i|\text{var.explicativa}_i) = 
\beta_0 + \beta_1\cdot f \\
H_1&:& \mathbb{E}(\text{var.endogena}_i|\text{var.explicativa}_i) = 
\mu(\text{var.explicativa})
\end{matrix}
\right.
$$
```{r results="hide", fig.show="hide"}
# La hipótesis alternativa la podemos expresar como una regresión de variables
# cualitativas:
var.explicativa.factor <- as.factor(var.explicativa)
ajuste.factor <- lm(var.endogena~var.explicativa.factor, data = datos)
anova(ajuste, ajuste.factor)

# Ilustración gráfica:
medias <- tapply(var.endogena, var.explicativa, mean) 
plot(var.explicativa, var.endogena, xlab="endógena",ylab="explicativa", 
     pch = 21, bg = "green", cex.lab=1.5, cex=0.9, cex.main=1.5)
# Ajuste bajo H_0
abline(ajuste, col="magenta", lwd=2)
# Ajuste bajo H_1
points(levels(var.explicativa.factor),medias,pch="-",cex=3,col="blue")
legend("topleft",c(expression(H[0]),expression(H[1])),
       col=c("magenta","blue"),lty=c(1,1), cex=1.5)
```
**Nota:** no se puede hacer si para muchas de las 
$x_i$ no tenemos varias $y_i$.

### Normalidad {#normalidad}
Gráficamente, a parte del QQ-Plot, se puede comprobar mediante el histograma,
densidad y diagrama de cajas de los residuos.
```{r results="hide"}
nortest::lillie.test(res.est) # Prueba de Lilliefors
# Para muestras pequeñas:
shapiro.test(res.est)         # Prueba de Shapiro-Wilks
moments::agostino.test(res.est)         # Prueba de D'Agostiino
# Para confirmar la influencia de las colas
nortest::ad.test(res.est)     # Prueba de Anderson-Darling
# Test no paramétrico
nortest::cvm.test(res.est)    # Prueba de Cramer Von-Mises

normtest::skewness.norm.test(res.est)   # Basada en asimetría
normtest::kurtosis.norm.test(res.est)   # Basada en kurtosis
normtest::jb.norm.test(res.est)         # prueba de Jarque-Bera (comprueba
# la asimetría y kurtosis a la vez)
```


### Aleatoriedad
```{r results="hide", fig.show="hide"}
Box.test(res.est, lag = 5, type = "Ljung-Box")	   	 # Prueba de Ljung-Box
acf(res.est, lag.max = 10, type = "correlation")$acf # Autocorrelaciones
tseries::runs.test(as.factor(sign(res.est)))	       # Prueba de rachas
```
**Nota:** solo tiene sentido realizarlo si tenemos almacenado cómo (orden 
cronológico) se han recogido los datos.

**Nota 2:** si no aceptamos la hipótesis de [normalidad](#normalidad)
no conseguimos nada aceptando la existencia de aleatoriedad.

### Homoscedasticidad
$$
\left\lbrace
\begin{matrix}
H_0&:& \sigma^2 = cte \\
H_1&:& \sigma^2 \neq cte
\end{matrix}
\right.
$$
```{r results="hide"}
# Test de Breusch-Pagan:
# H0: E(epsilon^2|X) = alpha0
# H1: E(epsilon^2|X) = alpha0 + alpha1X
car::ncvTest(ajuste)     # equivale a bptest(ajuste,studentize = FALSE)
lmtest::bptest(ajuste)   # LA RECOMENDACIÓN
```

## Transformaciones BoxCox
```{r}
lambda <- caret::BoxCoxTrans(var.endogena)
# BoxCoxTrans devuelve el lambda, por lo que tenemos que aplicarlo a 
# nuestra variable
var.endogena.transformada <- predict(lambda, var.endogena)
ajuste.transformado <- lm(var.endogena.transformada~var.explicativa)
```


## Análisis de influencia {#influence}
```{r results="hide"}
influencia <- influence(ajuste)
# Leverages (diagonales de la "matriz sombrero")
lev <- influencia$hat # también se pueden obtener usando hatvalues(ajuste)
# Distancias de Cook (gráfica influida en plot(ajuste))
distancia.cook <- cooks.distance(ajuste)

# DFFITS
dfajuste <- dffits(ajuste)
# Chequeamos sí:
sum(abs(dfajuste) > 2*sqrt(1/length(var.explicativa)))
# DFBETAS
dfbeta <- influencia$coefficients  # o dfbetas(ajuste)

# Cambio en la desviación estándar residual al eliminar cada punto
cambio.sd <- influencia$sigma
```


## Predicciones
```{r results="hide"}
# Intervalos de confianza al 90% para E(Y|X=x_i):
banda.conf.90 <- predict(ajuste, se.fit = TRUE,
                         interval = "confidence", level = 0.90)

# Intervalos de predicción al 90% para Y|X=x_i:
banda.pred.90 <- predict(ajuste, se.fit = TRUE, 
                         interval = "prediction", level = 0.90)
```

Si queremos predecir para unos valores concretos, debemos crear un dataframe
que contenga los valores de dónde queremos predecir y el nombre de la columna
ha de ser el nombre de la variable explicativa original (si no, no funciona):
```{r}
new.data <- data.frame(var.explicativa = valores.predecir)
IC.90 <- predict(ajuste, se.fit = TRUE, newdata = new.data,
                         interval = "confidence", level = 0.90)
```






# Regresión lineal múltiple


## Modelo matemático

$$
\vec{Y} = \vec{\beta} \mathbf{X} + \vec{\varepsilon}
$$

## Ajuste del modelo

#### Con todas las variables del conjunto de datos
```{r}
ajuste <- lm(var.endogena~., data=conjunto.datos)

coeficientes <- ajuste$coefficients; coeficientes <- coef(ajuste)
```

#### Con algunas variables del conjunto de datos
```{r}
ajuste <- lm(var.endogena~explicativa1+explicativa2+explicativa3,
             data=conjunto.datos)
```

#### Crear un nuevo modelo añadiendo/quitando variables
Ejemplo quitar las explicativas 1 y 2, añadir la 4 y la 5.
```{r}
ajuste2 <- update(ajuste, ~. -explicativa1 - explicativa2
                  +explicativa4 + explicativa5, data=conjunto.datos)
```



## Inferencia

### Intervalos de confianza

#### Para los parámetros individuales
```{r}
ci.individual <- confint(ajuste, level=0.95)
```

#### Para $\sigma^2$
```{r}
LS.IC.var <- gl.residuos * MSSR / qchisq(p=0.05, df=gl.residuos)   
```

#### Región de confianza

| Parámetros            | Significado                         |
|:---------------------:|:-----------------------------------:|
| **which.coef**        | Índices de los coeficientes a usar  |
| **levels**            | Vector con los niveles de confianza |

```{r}
confidenceEllipse(model=ajuste, which.coef=1:2, levels=c(0.80,0.90,0.95))
```


### Contrastes de Hipótesis

#### Contraste de Regresión:
$$
\begin{cases}
  H_0: \beta_1=\beta_2=\dots=\beta_k=0 \\
  H_1: \exists j \in \{1,\dots,k\}, \ \beta_j\neq 0
\end{cases}
$$
**Con summary**:

Es el estadístico F del resumen
```{r}
summary(modelo)
```

**A mano con anova**:

Modelo nulo (Bajo $H_0$)
```{r}
modelo.nulo <- lm(calidad ~ 1, data = conjunto.datos)
```

Modelo completo (Bajo $H_1$)
```{r}
modelo.completo <- ajuste
```

Contraste
```{r}
anova(modelo.nulo, modelo.completo)
```





## Bondad del ajuste:

### Coeficiente de determinación ($R^2$)
```{r}
summary(ajuste)$r.squared
```

### Coeficiente de determinación ajustado ($R^2_{adj}$)
```{r}
summary(ajuste)$adj.r.squared
```






## Diagnosis del modelo

## Análisis de influencia
Se realiza con las funciones ``influence`` y ``measures.influence()``

```{r}
influencia <- influence(ajustes)
```

```{r}
leverage <- influencia$hat # Valores de leverage
```

Si los datos tienen nombres en las filas se puede hacer una gráfica de leverage
e identificarlos haciendo click con la función ``identify`` y termiando
pulsando la tecla ``ESC``.
```{r}
plot(leverage)
identify(leverage, labels=row.names(conjunto.datos))
```


### DFBETAS
```{r}
DFBETAS <- influencia$coefficients
```

### Para obtener diferentes medidas diagnósticas
**(DFBETAS, DFFITS, COVRATIO y distancia D de Cook)**

Marca los puntos influyentes con un asterisco en la columna inf
```{r}
puntos.influyentes <- influence.measures(ajuste)
```

**Resumen de puntos influyentes:**
```{r}
summary(puntos.influyentes)
```

Para ver para cada dato del conjunto si es influyente (verdadero o falso) 
según los distintos criterios se puede utilizar:
```{r}
puntos.influyentes$is.inf
```


### Gráficas
**Librería ``olsrr``**
```{r}
# Distancia D de Cook
ols_plot_cooksd_bar(ajuste)
ols_plot_cooksd_chart(ajuste)
# DFFITS
ols_plot_dffits(ajuste)
#DFBETAs
ols_plot_dfbetas(ajuste)
# RESIDUOS ESTUDENTIZADOS
ols_plot_resid_stud(ajuste)
```





## Predicciones






## Criterios para la detección de multicolinealidad
```{r}
correlaciones <- cor(conjunto.datos)

det(correlaciones) # Si es muy cercano a cero hay alta multicolinealidad.
```


### Factores de Inflación de la Varianza
```{r}
FIV <- faraway::vif(ajuste)
```

Como criterio $\mathrm{FIV}_i > 10 \Longrightarrow$ Alta multicolinealidad.

Compararemos los FIV con $\dfrac{1}{1-R^2}$
```{r}
umbral <- 1/(1-summary(ajuste)$r.squared)
print(FIV[FIV>umbral])
```

### Índice de Condicionamiento

- $IC < 10 \Longrightarrow$ Ausencia de multicolinealidad

- $10 \leq IC \leq 30 \Longrightarrow$ Multicolinealidad moderada

- $IC > 30 \Longrightarrow$ Alta multicolinealidad

(Lo calcula la función ``mctest`` con ``type="o"`` (por defecto),
véase en el último apartado).


### Componentes principales
Comprobar si unas pocas componentes principales explican
una gran cantidad de varianza.
```{r}
pca <- prcomp(conjunto.datos, scale=T)
summary(pca)
```



### Análisis de multicolinealidad completo
| Parámetros          | Significado                           |
|:-------------------:|:-------------------------------------:|
| **type**            | "o" (por defecto) para todos los datos, "i" para las variables de forma individual |
| **method**          | Métodos a utilizar: ej: ``"VIF"``, ``"TOL"``  |
| **corr**            | Verdadero o Falso para mostrar la matriz de correlaciones junto a los resultados |

```{r}
mctest::mctest(ajuste, type="o")
mctest::mctest(ajuste, type="i", corr=T) 
mctest::mctest(ajuste, type="i", method = "VIF") 
mctest::mctest(ajuste, type="i", method = "TOL") 
```




## Criterios para selección de modelos

### Criterio MSS(R): ANOVA para modelos anidados
```{r}
modelo.completo <- lm(variable.endogena~., data=conjunto.datos)

# Modelo con menos variables
modelo.anidado <- lm(variable.endogena~explicativa1+explicativa2)

anova(modelo.anidado, modelo.completo)
```

### Criterio $C_p$ de Mallows

Cuanto más cercano sea $C_p$ a $p$ (y preferiblemente tomando valores
inferiores) mejor es el ajuste.

| Parámetros          | Significado                           |
|:-------------------:|:-------------------------------------:|
| **x**               | Matriz con las variables explicativas |
| **y**               | Vector con la variable respuesta      |

```{r}
Cp.residuos <- leaps::leaps(x=conjunto.datos[, 2:4],
                     y=conjunto.datos[, 1], method="Cp")
```

Una forma de ver los $C_p$ de Mallows como tabla, incluyendo columnas
que indican si la variable está presente en el modelo (1) o no (0) y el número
total de parámetros en el modelo es:
```{r}
cbind(Cp.residuos$which, p=Cp.residuos$size, Cp=Cp.residuos$Cp)
```

### Criterio de información de Akaike
Calcula la verosimilitud del modelo penalizando por el número de variables.
```{r}
AIC(ajuste)    # cuanto más pequeño, mejor (más ajustado)
```

### Criterio de Información Bayesiano (Schwarz)
A diferencia del de Akaike tiene en cuenta el número de datos que tenemos en
la muestra.
```{r}
BIC(ajuste)    # cuanto más pequeño, mejor (más ajustado)
```

### Estadístico PRESS
```{r}
DAAG::press(ajuste)   # cuanto más pequeño, mejor (más ajustado)
```






## Selección de variables
Se ha desarrollado el [algoritmo propuesto por Daniel Peña](algo.html) que tiene
muy encuenta la multicolinealidad.

### Comparar el modelo al eliminar una explicativa
El parámetro ``test="F" `` nos devuelve el valor del estadístico $F$
```{r}
drop1(ajuste, test="F")
```

### Comparar el modelo al añadir una explicativa

- En ``scope`` añadimos como rango inferior ``~.`` (el ajuste que ya tenemos)
Y las variables que queramos considerar añadir con +

- El parámetro ``test="F"`` nos devuelve el valor del estadístico $F$

```{r}
add1(ajuste, scope= ~.+explicativa.2+explicativa.3, test="F",
     data=conjunto.datos)
```

También podemos considerar añadir **cualquier variable del conjunto de datos**
usando el modelo completo.
```{r}
modelo.completo <- lm(var.endogena~., data=conjunto.datos)
add1(ajuste, scope=modelo.completo, test="F")
```


### Selección sistemática de un subconjunto de variables
```{r}
# Modelo que contiene solo el intercept
modelo.intercept <- lm(var.endogena~1, data=conjunto.datos)

# Modelo completo con todas las regresoras
modelo.completo <- lm(var.endogena~., data=conjunto.datos)
```


| Parámetros | Significado                           |
|:----------:|:-------------------------------------:|
| **object**     | Modelo de partida |
| **scope**      | lista de la forma ``list(lower=a, upper=b)`` con el rango de modelos anidados a considerar |
| **direction**  | modo de regresión paso a paso (``"both"`` (por defecto), ``"backward"`` o ``"forward"``)|
| **trace**      | 1 (por defecto) o 0 según se quiera o no ver los resultados intermedios |
| **k**          | Por defecto k=2 (indica AIC), si se pone k=log(n) se utiliza el criterio BIC, aunque en la salida seguirá poniendo AIC |
```{r}
step.modelo <- step(modelo.intercept,
                    scope=list(lower=modelo.intercept, upper=modelo.completo),
                    direction="both", trace=1)
```

Si no se especifica el parámetro **scope** se utiliza todo el rango de modelos pero la dirección utilizada será ``"backward"``
```{r}
step(modelo.completo)
```

**Resumen del proceso**
```{r}
step.modelo$anova
```

**Grados de libertad de la parte explicada por el modelo y AIC**
```{r}
extractAIC(step.modelo)
```


### Eliminación progresiva de variables
```{r}
step.backward <- step(modelo.completo, direction="backward", trace=1)
```


### Introducción progresiva de variables
```{r}
step.forward <- step(modelo.intercept,
                     scope=list(lower=modelo.intercept, upper=modelo.completo),
                     direction="forward", trace=1)
```

### Búsqueda exhaustiva con la librería leaps

| Parámetros  | Significado                           |
|:-----------:|:-------------------------------------:|
| **nbest**   | Número de modelos a seleccionar para cada número de variables, ej: con 1 se devuelve el mejor subconjunto con una sola variable|
| **nvmax**   | Número máximo de variables a considerar  |
```{r}
modelo.exhaustivo <- leaps::regsubsets(var.endogena ~.,
                                       data=conjunto.datos, nbest=1)
```

Para ver los resultados, emplear ``summary()``
```{r}
summary(modelo.exhaustivo)
```

Además summary() tiene los siguientes atributos:

| Parámetros  | Significado                         |
|:-----------:|:-----------------------------------:|
| **rsg**     | $R^2$ de cada modelo                |
| **adjr2**   | $R^2_{adj}$ de cada modelo          | 
| **cp**      | $C_p$ de Mallows de cada modelo     |
| **bic**     | $\mathrm{BIC}$ de cada modelo       |
| **rss**     | $\mathrm{MSS(R)}$ de cada modelo    |

Viendo los datos en una salida:
```{r}
S <- summary(modelo.exhaustivo)

cbind(S$which, round(cbind(rsq=S$rsq, adjr2=S$adjr2, cp=S$cp, bic=S$bic, 
                           rss=S$rss), 3))
```

Leaps incluye también plots
```{r}
plot(modelo.exhaustivo, scale="adjr2")
plot(modelo.exhaustivo, scale="bic")
plot(modelo.exhaustivo, scale="Cp")
plot(modelo.exhaustivo, scale="r2")
```

Nótese que el atributo de ``summary`` para el $C_p$ de Mallows tiene c minúscula
y el del plot del modelo mayúscula

### Vías de evaluación

### Coeficiente de robustez
$$
B^2 = \dfrac{\mathrm{SS(R)}}{\mathrm{PRESS}} \qquad \qquad B^2 \in (0,1)
$$
```{r}
B2 <- sum(residuals(ajuste)^2/DAAG::press(ajuste))
```


### Validación Cruzada



# Regresión logística
## prueba
### más pruebas
