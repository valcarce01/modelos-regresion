---
title: "Modelos de regresión"
output: 
  prettydoc::html_pretty:
    theme: leonids
    fig_caption: yes
    highlight: tango
    toc: yes
    fig_width: 10 
    fig_height: 5
---
Renuncia de responsibilidad.

# Regresión lineal simple
Objetivo: construir un modelo tal que $Y_i = \beta_0 + \beta_1\cdot X_i + \epsilon_i$

```{r include = FALSE}
data("iris")
var.endogena <- iris[,1]
var.explicativa <- iris[,2]
datos <- data.frame(cbind(var.endogena, var.explicativa))
library(DAAG)
library(stats)
library(MASS)
library(car)
library(lmtest)
library(tseries)
```


## Construcción del modelo
```{r}
ajuste <- lm(var.endogena~var.explicativa, data = datos)

coeficientes <- ajuste$coefficients
valores.ajustados <- ajuste$fitted.values
residuos <- ajuste$residuals
gl.residuos <- ajuste$df.residual # grados de libertad de los residuos
gl.modelo <- ajuste$rank          # grados de libertad del modelo
SSR <- sum(residuos^2)            # suma de residuos al cuadrado
MSSR <- SSR/gl.residuos           # media de SSR
leverage <- hatvalues(ajuste)
n.equivalente.datos <- 1/leverage
```

## Inferencia
### Intervalos de confianza
```{r}
intervalo.parametros <- confint(ajuste, level = 0.95)
intervalo.varianza <- gl.residuos * MSSR / qchisq(p=0.10,df=gl.residuos)
```
### Contrastes de hipótesis
```{r}
# Contraste de regresión (H_0: beta_1 = 0):
resumen <- summary(ajuste)
contraste <- resumen$coefficients
# en la última columna tenemos el contraste de regrsión y el contraste de
# beta_0 = 0

# Podemos acceder al resultado del mismo contraste mediante anova:
contraste.regresion <- anova(ajuste)
```

## Bondad del ajuste
```{r results = 'hide', fig.show='hide'}
# Coeficiente de correlación lineal de Pearson estimado:
r <- cor(var.endogena, var.explicativa)
# ¿Es significativamente diferente de cero? H0:rho=0 vs H1:rho!=0
cor.test(var.endogena, var.explicativa, method = "pearson")

# Coeficiente de determinación
R2 <- resumen$r.squared
R2.ajustado <- resumen$adj.r.squared

# AIC, Criterio de Información de Akaike
AIC(ajuste)    # cuanto más pequeño, mejor (más ajustado)
# BIC, Criterio de Información Bayesiano (lo mismo, pero teniendo en cuenta
# el número de datos que tenemos)
BIC(ajuste)    # cuanto más pequeño, mejor (más ajustado)

# Validación cruzada
# hace un plot además con los 'distintos' modelos creados
cvRes <- DAAG::CVlm(data=datos, form.lm=var.endogena~var.explicativa,
                    m=5, dots=FALSE, seed=29, plotit="Observed", 
                    legend.pos="topleft", 
                    printit = FALSE, main="")

```

## Diagnosis del modelo
```{r}
res.est <- MASS::stdres(ajuste)    # estudentizados internamente (o "estandarizados") 
```

Vamos a corroborar las hipótesis estructurales del modelo: linealidad, homoscedasticidad
normalidad y aleatoriedad. Con `plot(ajuste)` tenemos acceso a diversas gráficas
que pueden ser de ayuda:

||¿Qué representa?|¿Qué comprobamos?|
|-------|:-------:|:-----:|
|`plot(ajuste, which = 1)`|Valores ajustados ~ Residuos|Linealidad|
|`plot(ajuste, which = 2)`|QQ Plot|Normalidad|
|`plot(ajuste, which = 3)`|Valores ajustados ~ Residuos estandarizados|Homoscedasticidad|
|`plot(ajuste, which = 4)`|Distancias de Cook|[Datos influyentes](#influence)|
|`plot(ajuste, which = 5)`|Leverage ~ Residuos estandarizados|[Datos influyentes](#influence)|

Pruebas analíticas:

### Linealidad
Contraste de linealidad. 
$$
\left\lbrace
\begin{matrix}
H_0&:& \mathbb{E}(\text{var.endogena}_i|\text{var.explicativa}_i) = 
\beta_0 + \beta_1\cdot f \\
H_1&:& \mathbb{E}(\text{var.endogena}_i|\text{var.explicativa}_i) = 
\mu(\text{var.explicativa})
\end{matrix}
\right.
$$
```{r results="hide", fig.show="hide"}
# La hipótesis alternativa la podemos expresar como una regresión de variables
# cualitativas:
var.explicativa.factor <- as.factor(var.explicativa)
ajuste.factor <- lm(var.endogena~var.explicativa.factor, data = datos)
anova(ajuste, ajuste.factor)

# Ilustración gráfica:
medias <- tapply(var.endogena, var.explicativa, mean) 
plot(var.explicativa, var.endogena, xlab="endógena",ylab="explicativa", 
     pch = 21, bg = "green", cex.lab=1.5, cex=0.9, cex.main=1.5)
# Ajuste bajo H_0
abline(ajuste, col="magenta", lwd=2)
# Ajuste bajo H_1
points(levels(var.explicativa.factor),medias,pch="-",cex=3,col="blue")
legend("topleft",c(expression(H[0]),expression(H[1])),
       col=c("magenta","blue"),lty=c(1,1), cex=1.5)
```
**Nota:** no se puede hacer si para muchas de las 
$x_i$ no tenemos varias $y_i$.

### Normalidad {#normalidad}
Gráficamente, a parte del QQ-Plot, se puede comprobar mediante el histograma,
densidad y diagrama de cajas de los residuos.
```{r results="hide"}
nortest::lillie.test(res.est) # Prueba de Lilliefors
# Para muestras pequeñas:
shapiro.test(res.est)         # Prueba de Shapiro-Wilks
moments::agostino.test(res.est)         # Prueba de D'Agostiino
# Para confirmar la influencia de las colas
nortest::ad.test(res.est)     # Prueba de Anderson-Darling
# Test no paramétrico
nortest::cvm.test(res.est)    # Prueba de Cramer Von-Mises

normtest::skewness.norm.test(res.est)   # Basada en asimetría
normtest::kurtosis.norm.test(res.est)   # Basada en kurtosis
normtest::jb.norm.test(res.est)         # prueba de Jarque-Bera (comprueba
# la asimetría y kurtosis a la vez)
```


### Aleatoriedad
```{r results="hide", fig.show="hide"}
Box.test(res.est, lag = 5, type = "Ljung-Box")	   	 # Prueba de Ljung-Box
acf(res.est, lag.max = 10, type = "correlation")$acf # Autocorrelaciones
tseries::runs.test(as.factor(sign(res.est)))	       # Prueba de rachas
```
**Nota:** solo tiene sentido realizarlo si tenemos almacenado cómo (orden 
cronológico) se han recogido los datos.

**Nota 2:** si no aceptamos la hipótesis de [normalidad](#normalidad)
no conseguimos nada aceptando la existencia de aleatoriedad.

### Homoscedasticidad
$$
\left\lbrace
\begin{matrix}
H_0&:& \sigma^2 = cte \\
H_1&:& \sigma^2 \neq cte
\end{matrix}
\right.
$$
```{r results="hide"}
# Test de Breusch-Pagan:
# H0: E(epsilon^2|X) = alpha0
# H1: E(epsilon^2|X) = alpha0 + alpha1X
car::ncvTest(ajuste)     # equivale a bptest(ajuste,studentize = FALSE)
lmtest::bptest(ajuste)   # LA RECOMENDACIÓN
```


## Análisis de influencia {#influence}
## Predicciones


# Regresión lineal múltiple

# Regresión logística
## prueba
### más pruebas
